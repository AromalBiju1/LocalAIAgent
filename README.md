# LocalAI Agent Framework

> A production-ready, zero-cost AI agent system running entirely on local hardware. Currently featuring a complete LLM interface module, with RAG, tool calling, and multi-agent orchestration planned.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Project Status

### âœ… Complete - LLM Interface Module
The core LLM module is **production-ready** and provides:

- âœ… **Unified API** for Ollama and llama.cpp backends
- âœ… **Async/Sync** support for maximum flexibility
- âœ… **Streaming** responses for better UX
- âœ… **Type-safe** with proper abstractions
- âœ… **Well-tested** with comprehensive test suite
- âœ… **Production-ready** with error handling and logging
- âœ… **CLI interface** for interactive chat and one-shot queries

### ğŸš§ Planned - Future Modules
- ğŸ”² **RAG System**: Vector search with FAISS/Qdrant + semantic chunking
- ğŸ”² **Tool Calling**: Extensible tool system with automatic function discovery
- ğŸ”² **Multi-Agent**: Specialized agents for different tasks (research, coding, analysis)
- ğŸ”² **Memory Management**: Conversation history with intelligent summarization
- ğŸ”² **API Server**: FastAPI REST + WebSocket endpoints
- ğŸ”² **Advanced CLI**: Rich terminal UI with progress indicators

## ğŸ—ï¸ Architecture

### Current Implementation (âœ… Complete)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your Application                          â”‚
â”‚                   (Build on top of this)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              âœ… LLM Interface Module (Complete)              â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  BaseLLM (Abstract Interface)                        â”‚  â”‚
â”‚  â”‚    â€¢ Message, MessageRole, LLMResponse               â”‚  â”‚
â”‚  â”‚    â€¢ StreamChunk for streaming                       â”‚  â”‚
â”‚  â”‚    â€¢ Sync/Async operations                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OllamaLLM       â”‚      LlamaCppLLM                 â”‚  â”‚
â”‚  â”‚  â€¢ HTTP API      â”‚      â€¢ Direct GGUF loading       â”‚  â”‚
â”‚  â”‚  â€¢ Auto models   â”‚      â€¢ GPU control               â”‚  â”‚
â”‚  â”‚  â€¢ Streaming     â”‚      â€¢ Thread mgmt               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLMFactory (Easy Instantiation)                     â”‚  â”‚
â”‚  â”‚    â€¢ create() - From parameters                      â”‚  â”‚
â”‚  â”‚    â€¢ from_config() - From YAML                       â”‚  â”‚
â”‚  â”‚    â€¢ Backend detection & recommendation              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Inference Backends                        â”‚
â”‚          Ollama Server    â”‚    Llama.cpp (Direct)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Planned Architecture (ğŸš§ Future)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FastAPI Server + WebSocket                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent Orchestrator                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Research   â”‚  â”‚    Coding    â”‚  â”‚   Analysis   â”‚     â”‚
â”‚  â”‚    Agent     â”‚  â”‚    Agent     â”‚  â”‚    Agent     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… LLM Module  â”‚  ğŸš§ RAG System  â”‚  ğŸš§ Tool Registry      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Vector DB  â”‚  Conversation Store  â”‚  Document Cache     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Clone or download this project
cd localai-agent

# Run setup script
chmod +x setup.sh
./setup.sh

# Or manually:
pip install -r requirements.txt
```

### 2. Start Ollama

```bash
# Start Ollama server
ollama serve

# Pull a model
ollama pull qwen3:4b
```

### 3. Test It!

```bash
# Run examples
python examples/llm_examples.py

# Try the CLI
python llm_cli.py "What is machine learning?"

# Interactive chat
python llm_cli.py --interactive
```

## ğŸ“– Usage Examples

### Basic Generation

```python
from src.core import LLMFactory, Message, MessageRole

# Create LLM
llm = LLMFactory.create(backend="ollama", model_name="qwen3:4b")

# Create message
messages = [
    Message(role=MessageRole.SYSTEM, content="You are a helpful assistant."),
    Message(role=MessageRole.USER, content="Explain Python in one sentence.")
]

# Generate
response = llm.generate(messages)
print(response.content)
```

### Streaming

```python
# Stream the response
for chunk in llm.stream(messages):
    print(chunk.content, end='', flush=True)
```

### Async

```python
import asyncio

async def main():
    llm = LLMFactory.create(backend="ollama", model_name="qwen3:4b")
    
    # Async generate
    response = await llm.agenerate(messages)
    print(response.content)
    
    # Async stream
    async for chunk in llm.astream(messages):
        print(chunk.content, end='', flush=True)

asyncio.run(main())
```

### Configuration

```python
import yaml

# Load from config file
with open('config/config.yaml') as f:
    config = yaml.safe_load(f)

llm = LLMFactory.from_config(config['model']['ollama'])
```

## ğŸ—ï¸ Project Structure

```
localai-agent/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ core/              # LLM module
â”‚       â”œâ”€â”€ llm_base.py    # Abstract base class
â”‚       â”œâ”€â”€ ollama_llm.py  # Ollama implementation
â”‚       â”œâ”€â”€ llamacpp_llm.py# Llama.cpp implementation
â”‚       â”œâ”€â”€ llm_factory.py # Factory pattern
â”‚       â””â”€â”€ __init__.py    # Public API
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml        # Configuration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_llm.py        # Test suite
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ llm_examples.py    # Usage examples
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ LLM_MODULE.md      # Detailed documentation
â”œâ”€â”€ llm_cli.py             # CLI interface
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ setup.sh              # Setup script
â””â”€â”€ README.md             # This file
```

## ğŸ”§ Configuration

Edit `config/config.yaml`:

```yaml
model:
  backend: ollama          # or 'llamacpp'
  name: qwen3:4b
  
  ollama:
    base_url: http://localhost:11434
    timeout: 300
  
  llamacpp:
    model_path: /path/to/model.gguf
    n_ctx: 4096
    n_gpu_layers: -1       # -1 = use all GPU

generation:
  temperature: 0.7
  max_tokens: 2048
  top_p: 0.9
```

## ğŸ® CLI Usage

```bash
# One-shot question
python llm_cli.py "What is Python?"

# Streaming response
python llm_cli.py --stream "Tell me a story"

# Interactive chat
python llm_cli.py --interactive

# Adjust temperature
python llm_cli.py --temperature 1.2 "Write a poem"

# List available models
python llm_cli.py --list-models
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/test_llm.py -v

# Run with coverage
pytest tests/test_llm.py --cov=src/core --cov-report=html

# Run specific test
pytest tests/test_llm.py::TestOllamaLLM::test_generate -v
```

## ğŸ“Š Hardware Requirements

**Minimum (for 4B models):**
- GPU: 4GB VRAM
- RAM: 8GB
- CPU: Any modern quad-core

**Recommended (your setup):**
- âœ… GPU: 6GB VRAM (RTX 4050)
- âœ… RAM: 16GB DDR5
- âœ… CPU: i5-13420H

**Performance on your hardware:**
- Qwen 3 4B: ~45-60 tokens/sec
- Memory usage: ~4-5GB total
- Can run 2-3 concurrent requests

## ğŸ”„ Supported Backends

### Ollama (Recommended)

**Pros:**
- Easy setup
- Automatic model management
- Active development
- Good performance

**Install:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
ollama pull qwen3:4b
```

### Llama.cpp

**Pros:**
- Direct model loading
- Fine-grained control
- Lower overhead

**Install:**
```bash
pip install llama-cpp-python
# Download GGUF model from HuggingFace
```

## ğŸ“š Documentation

- [LLM Module Documentation](docs/LLM_MODULE.md) - Detailed guide
- [API Reference](src/core/) - Code documentation
- [Examples](examples/llm_examples.py) - Usage examples

## ğŸ¯ What You Can Build Right Now

With the current LLM module, you can already build:

### âœ… Currently Possible
- **Chatbots**: Interactive conversations with context
- **Text Generation**: Content creation, summaries, explanations
- **Code Assistance**: Code generation, explanation, debugging
- **Q&A Systems**: Answer questions based on prompts
- **Text Analysis**: Sentiment, classification, extraction
- **Translation**: Multi-language translation
- **Creative Writing**: Stories, poems, articles

### ğŸš€ Example Use Cases
```python
# Multi-turn conversation bot
from src.core import LLMFactory, Message, MessageRole

llm = LLMFactory.create(backend="ollama", model_name="qwen3:4b")
messages = [Message(MessageRole.SYSTEM, "You are a helpful coding assistant.")]

while True:
    user_input = input("You: ")
    messages.append(Message(MessageRole.USER, user_input))
    
    response = llm.generate(messages, temperature=0.7)
    print(f"Assistant: {response.content}")
    
    messages.append(Message(MessageRole.ASSISTANT, response.content))
```

### ğŸ”œ Coming Soon (With Future Modules)
- **RAG Applications**: Search your documents and get AI-powered answers
- **AI Agents**: Autonomous task completion with tool use
- **Code Execution**: Run generated code and iterate
- **Web Search Integration**: Get current information
- **File Processing**: Analyze PDFs, documents, spreadsheets

## ğŸ›£ï¸ Roadmap

This is the **LLM foundation**. Next modules:

- [ ] **RAG System** - Vector search, embeddings, document retrieval
- [ ] **Tool Calling** - Function calling, external tool integration
- [ ] **Agents** - Specialized agents (research, coding, analysis)
- [ ] **Memory** - Conversation history, summarization
- [ ] **API Server** - FastAPI REST + WebSocket
- [ ] **CLI** - Interactive terminal interface
- [ ] **Monitoring** - Prometheus metrics, logging

## ğŸ¤ Contributing

Contributions welcome! This is a modular system, so you can:

1. Add new LLM backends
2. Improve existing implementations
3. Add tests
4. Improve documentation
5. Report issues

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file

## ğŸ™ Acknowledgments

- **Qwen Team** - For the excellent Qwen models
- **Ollama Team** - For making local LLMs accessible
- **llama.cpp** - For efficient inference

## ğŸ’¡ Tips

1. **Start with Ollama** - It's the easiest to set up
2. **Use 4B models** - Perfect for 6GB GPU
3. **Enable streaming** - Better user experience
4. **Monitor memory** - Keep context under 4K tokens
5. **Read the docs** - Check `docs/LLM_MODULE.md` for details

## ğŸ› Troubleshooting

**Ollama not connecting?**
```bash
# Check if running
curl http://localhost:11434/api/tags

# Start it
ollama serve
```

**Out of memory?**
- Use smaller model (qwen3:4b)
- Reduce context window
- Close other GPU applications

**Slow responses?**
- Ensure GPU is being used
- Check `nvidia-smi` for GPU utilization
- Try quantized models

**Import errors?**
```bash
pip install -r requirements.txt
```

## ğŸ“§ Contact

**Author**: Aromal Biju  
**Purpose**: Building efficient local AI agents  
**Hardware**: 6GB GPU + i5-13420H + 16GB RAM

---

**Ready to build more?** This LLM module is just the foundation. Next up: RAG, Tools, and Agents! ğŸš€
